# -*- coding: utf-8 -*-
"""CNN Object Recognition in Images.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w2cQIqagCab7DEdvX_DokNW7bjiFT79p

# Step 1: Installation and setup
"""

!pip install --upgrade pip setuptools

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# Step 2: Data Preprocessing"""

# Importing the dataset
# cifar10 is a build in dataset in tensorflow.keras.datasets
from tensorflow.keras.datasets import cifar10

"""dataset is also available on kaggle https://www.kaggle.com/c/cifar-10/data"""

# Loading the dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# After downloading this data
# create a variable for all the classes in data

class_names = ['0: airplane', '1: automobile','2: bird','3: cat','4: deer', '5: dog', '6: frog','7: horse','8: ship','9: truck']

print(class_names)

x_train.max(), x_train.min(), x_train.mean()

"""We can say that range of the dataset is from 0 to 255"""

y_train.max(), y_train.min()

class_names

# Normalizing the images
# After normalization we can process all these images faster

x_train = x_train / 255.0
x_test = x_test / 255.0

x_train.max(), x_train.min(), x_train.mean()

x_train.shape, x_test.shape

"""There are total 50000 images in this dataset as training set and 10000 as test set"""

plt.imshow(x_train[0])

y_train[0]

class_names

"""This is a frog

# Step 3: Building the CNN
"""

# Defining the object
model = tf.keras.models.Sequential()

# Adding first CNN layer
# 1) filters = 32 (kernel)
# 2) kernal size = 3 (filter size)
# 3) padding = same
# 4) activation = ReLU
# 5) input shape = (32, 32, 3)

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding = 'same', activation='relu',input_shape = [32,32,3]))

# Adding second CNN layer and maxpool layer
# in second layer we dont have to specify the input shape

model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding = 'same', activation='relu'))

# maxpool layer parameters
# 1) pool size = 2
# 2) strides = 2
# 3) padding = valid

model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2,padding='valid'))

# Adding third CNN layer
# 1) filters (kernel) = 64
# 2) kernel size = 3
# 3)  padding = same
# 4) activation = ReLU

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding = 'same', activation='relu'))

# Adding fourth CNN layer and maxpool layer
# in second layer we dont have to specify the input shape

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding = 'same', activation='relu'))

# maxpool layer parameters
# 1) pool size = 2
# 2) strides = 2
# 3) padding = valid

model.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2,padding='valid'))

# Adding the dropout layer
# Doing Regularisation by Dropout

model.add(tf.keras.layers.Dropout(0.4))

# Adding the Flattening layer
model.add(tf.keras.layers.Flatten())

# Adding first dense layer
model.add(tf.keras.layers.Dense(units=128, activation='relu'))

# Adding sencond dense layer
model.add(tf.keras.layers.Dense(units=128, activation='softmax'))

model.summary()

"""# Step 4: Training the model"""

# Compile model

model.compile(loss= 'sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])

# metrics calculate accuracy

# Training the model

model.fit(x_train, y_train, batch_size=10, epochs=10)

"""# Step 5: Model evaluation and prediction"""

# evaluate the model performace
test_loss, test_acc = model.evaluate(x_test,y_test)

# Predictions

y_pred = np.argmax(model.predict(x_test), axis=-1)

print(y_pred[10]), print(y_test[10])

print(y_pred[100]), print(y_test[100])

# Confusion matrix

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)

# Calculating the accuracy on the basis on confusion matrix

acc_cm = accuracy_score(y_test, y_pred)
print(acc_cm)

